# Handling Data Heterogeneity in Medical Image Classification

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

A comprehensive implementation of federated learning techniques for medical image classification, specifically designed to handle data heterogeneity across distributed healthcare institutions while preserving patient privacy.

## ğŸ¯ Overview

This project addresses the critical challenge of data heterogeneity in federated learning for medical image classification. In real-world healthcare scenarios, different hospitals and medical centers often have:

- **Non-IID data distributions** (varying patient demographics, disease prevalence)
- **Different imaging equipment** (various manufacturers, protocols, resolutions)
- **Imbalanced datasets** (unequal class distributions across clients)
- **Privacy constraints** (strict data protection requirements)

Our solution implements state-of-the-art federated learning algorithms specifically tailored for these challenges.

## ğŸ—ï¸ Architecture

The federated learning system consists of:

- **Central Server**: Coordinates the training process and aggregates model updates
- **Client Nodes**: Local training on hospital/institution data
- **Communication Protocol**: Secure exchange of model parameters (not raw data)
- **Aggregation Strategies**: Advanced techniques to handle heterogeneous updates

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Central Server â”‚
â”‚   (Aggregator)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”
    â”‚     â”‚     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Client1â”‚ â”‚ â”‚Client3â”‚
â”‚(Hosp.A)â”‚ â”‚ â”‚(Hosp.C)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜
        â”Œâ”€â–¼â”€â”€â”€â”
        â”‚Client2â”‚
        â”‚(Hosp.B)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”¬ Key Features

### Data Heterogeneity Handling
- **FedAvg**: Standard federated averaging baseline
- **FedProx**: Proximal term to handle system heterogeneity
- **FedNova**: Normalized averaging for non-IID data
- **SCAFFOLD**: Control variates for unbiased updates
- **FedBN**: Batch normalization adaptation for statistical heterogeneity

### Privacy Preservation
- **Differential Privacy**: Noise injection for enhanced privacy
- **Secure Aggregation**: Cryptographic protection of model updates
- **Homomorphic Encryption**: Computation on encrypted data

### Medical Image Support
- **DICOM Processing**: Native support for medical imaging standards
- **Multi-modal Data**: Support for CT, MRI, X-ray, ultrasound
- **3D Volume Processing**: Specialized handling for volumetric medical data
- **Data Augmentation**: Medical-specific augmentation techniques

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-capable GPU (recommended)
- 8GB+ RAM

### Setup Environment

```bash
# Clone the repository
git clone https://github.com/allirezamaleki/Handling-Data-Heterogeneity-in-Medical-Image-Classification.git
cd Handling-Data-Heterogeneity-in-Medical-Image-Classification

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Optional: Install development dependencies
pip install -r requirements-dev.txt
```

### Docker Setup (Alternative)

```bash
# Build Docker image
docker build -t federated-medical-classification .

# Run container
docker run --gpus all -v $(pwd):/workspace federated-medical-classification
```

## ğŸš€ Quick Start

### 1. Data Preparation

```python
from data.data_loader import MedicalDataLoader
from data.preprocessor import DicomPreprocessor

# Load and preprocess medical images
loader = MedicalDataLoader(data_path="./datasets")
preprocessor = DicomPreprocessor()

# Create federated data splits
federated_data = loader.create_federated_splits(
    num_clients=5,
    distribution='non_iid',
    alpha=0.5  # Dirichlet parameter for non-IIDness
)
```

### 2. Configure Federated Learning

```python
from federated.config import FederatedConfig
from federated.server import FederatedServer
from federated.client import FederatedClient

# Configuration
config = FederatedConfig(
    algorithm='FedAvg',
    num_rounds=100,
    num_clients=5,
    clients_per_round=3,
    local_epochs=5,
    learning_rate=0.001,
    batch_size=32
)

# Initialize server and clients
server = FederatedServer(config)
clients = [FederatedClient(client_id=i, data=federated_data[i]) 
           for i in range(config.num_clients)]
```

### 3. Run Training

```python
from federated.trainer import FederatedTrainer

trainer = FederatedTrainer(server, clients, config)
results = trainer.train()

# Evaluate global model
test_accuracy = trainer.evaluate_global_model()
print(f"Final Test Accuracy: {test_accuracy:.4f}")
```

## ğŸ“Š Supported Datasets

### Medical Image Datasets
- **ChestX-ray14**: Chest X-ray pathology classification
- **ISIC 2019**: Skin lesion classification
- **COVID-19 CT**: COVID-19 detection from CT scans
- **HAM10000**: Dermatoscopic images of skin lesions
- **Brain Tumor MRI**: Multi-class brain tumor classification

### Data Distribution Scenarios
- **IID**: Independent and identically distributed data
- **Non-IID (Label)**: Uneven class distributions across clients
- **Non-IID (Feature)**: Different imaging conditions/equipment
- **Pathological Non-IID**: Extreme heterogeneity scenarios

## âš™ï¸ Configuration

### Algorithm Parameters

```yaml
# config/federated_config.yaml
federated_learning:
  algorithm: "FedAvg"  # Options: FedAvg, FedProx, FedNova, SCAFFOLD
  num_rounds: 100
  num_clients: 10
  clients_per_round: 5
  local_epochs: 5
  
model:
  architecture: "ResNet18"  # Options: ResNet18/50, DenseNet121, EfficientNet
  pretrained: true
  num_classes: 14
  
training:
  learning_rate: 0.001
  batch_size: 32
  optimizer: "Adam"
  weight_decay: 1e-4
  
privacy:
  differential_privacy: false
  noise_multiplier: 1.0
  max_grad_norm: 1.0
```

### Client Heterogeneity Simulation

```python
# Simulate different types of heterogeneity
heterogeneity_configs = {
    'label_skew': {
        'type': 'dirichlet',
        'alpha': 0.5,  # Lower alpha = higher heterogeneity
    },
    'feature_skew': {
        'type': 'noise_injection',
        'noise_level': 0.1,
    },
    'quantity_skew': {
        'type': 'power_law',
        'exponent': 1.5,
    }
}
```

## ğŸ“ˆ Experimental Results

### Performance Comparison

| Algorithm | IID Accuracy | Non-IID (Î±=0.5) | Non-IID (Î±=0.1) | Communication Rounds |
|-----------|-------------|------------------|------------------|---------------------|
| FedAvg    | 87.3%       | 82.1%           | 76.8%           | 100                |
| FedProx   | 87.5%       | 84.2%           | 79.3%           | 100                |
| FedNova   | 87.8%       | 85.1%           | 81.2%           | 100                |
| SCAFFOLD  | 88.1%       | 85.8%           | 82.7%           | 100                |

### Convergence Analysis

```python
# Plot training curves
from utils.visualization import plot_convergence

plot_convergence(
    results_dict={
        'FedAvg': fedavg_results,
        'FedProx': fedprox_results,
        'SCAFFOLD': scaffold_results
    },
    save_path='./results/convergence_comparison.png'
)
```

## ğŸ”§ Advanced Usage

### Custom Algorithm Implementation

```python
from federated.algorithms.base import FederatedAlgorithm

class CustomFedAlgorithm(FederatedAlgorithm):
    def __init__(self, config):
        super().__init__(config)
        
    def aggregate(self, client_updates):
        # Implement custom aggregation logic
        return aggregated_model
        
    def client_update(self, model, local_data):
        # Implement custom client update
        return updated_model, metrics
```

### Privacy-Preserving Extensions

```python
from privacy.differential_privacy import DPFederatedLearning
from privacy.secure_aggregation import SecureAggregator

# Enable differential privacy
dp_config = {
    'noise_multiplier': 1.0,
    'max_grad_norm': 1.0,
    'target_epsilon': 10.0
}

trainer = DPFederatedLearning(config, dp_config)
```

## ğŸ“‹ Project Structure

```
â”œâ”€â”€ data/                     # Data loading and preprocessing
â”‚   â”œâ”€â”€ data_loader.py       # Federated data loading utilities
â”‚   â”œâ”€â”€ preprocessor.py      # Medical image preprocessing
â”‚   â””â”€â”€ augmentation.py      # Medical-specific data augmentation
â”œâ”€â”€ federated/               # Federated learning core
â”‚   â”œâ”€â”€ algorithms/          # FL algorithm implementations
â”‚   â”œâ”€â”€ server.py           # Central server logic
â”‚   â”œâ”€â”€ client.py           # Client-side training
â”‚   â””â”€â”€ aggregation.py      # Model aggregation strategies
â”œâ”€â”€ models/                  # Neural network architectures
â”‚   â”œâ”€â”€ resnet.py           # ResNet implementations
â”‚   â”œâ”€â”€ densenet.py         # DenseNet implementations
â”‚   â””â”€â”€ efficientnet.py     # EfficientNet implementations
â”œâ”€â”€ privacy/                 # Privacy-preserving techniques
â”‚   â”œâ”€â”€ differential_privacy.py
â”‚   â””â”€â”€ secure_aggregation.py
â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”œâ”€â”€ metrics.py          # Evaluation metrics
â”‚   â”œâ”€â”€ visualization.py    # Plotting and visualization
â”‚   â””â”€â”€ config.py           # Configuration management
â”œâ”€â”€ experiments/             # Experimental scripts
â”‚   â”œâ”€â”€ run_experiment.py   # Main experiment runner
â”‚   â””â”€â”€ benchmark.py        # Benchmarking utilities
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ config/                  # Configuration files
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Container setup
â””â”€â”€ README.md               # This file
```

## ğŸ§ª Running Experiments

### Benchmark Comparison

```bash
# Run comprehensive benchmark
python experiments/benchmark.py \
    --dataset ChestXray14 \
    --algorithms FedAvg,FedProx,SCAFFOLD \
    --num_clients 10 \
    --alpha 0.1,0.5,1.0 \
    --rounds 100

# Specific algorithm test
python experiments/run_experiment.py \
    --config config/fedavg_config.yaml \
    --output_dir ./results/fedavg_experiment
```

### Custom Dataset Integration

```python
from data.custom_dataset import CustomMedicalDataset

# Implement your custom dataset
class MyMedicalDataset(CustomMedicalDataset):
    def __init__(self, data_path, transform=None):
        super().__init__(data_path, transform)
        
    def load_data(self):
        # Custom data loading logic
        pass
        
    def get_federated_splits(self, num_clients):
        # Custom federated splitting
        pass
```

## ğŸ“Š Evaluation Metrics

### Medical-Specific Metrics
- **AUC-ROC**: Area under ROC curve for each class
- **Sensitivity/Recall**: True positive rate (crucial for medical diagnosis)
- **Specificity**: True negative rate
- **F1-Score**: Harmonic mean of precision and recall
- **Cohen's Kappa**: Inter-rater reliability measure

### Federated Learning Metrics
- **Communication Efficiency**: Rounds to convergence
- **Client Fairness**: Performance variance across clients
- **Privacy Loss**: Differential privacy epsilon values
- **Robustness**: Performance under various attacks

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Code formatting
black . --line-length 88
isort .

# Linting
flake8 .
```

### Submitting Issues

When reporting bugs or requesting features, please include:
- Python and PyTorch versions
- Hardware specifications (GPU model, RAM)
- Complete error traceback
- Minimal reproducible example

## ğŸ“š References

### Key Papers
1. McMahan, B., et al. (2017). "Communication-efficient learning of deep networks from decentralized data." AISTATS.
2. Li, T., et al. (2020). "Federated optimization in heterogeneous networks." MLSys.
3. Karimireddy, S.P., et al. (2020). "SCAFFOLD: Stochastic controlled averaging for federated learning." ICML.
4. Wang, J., et al. (2020). "Tackling the objective inconsistency problem in heterogeneous federated optimization." NeurIPS.

### Medical AI & Privacy
- Kaissis, G.A., et al. (2020). "Secure, privacy-preserving and federated machine learning in medical imaging." Nature Machine Intelligence.
- Rieke, N., et al. (2020). "The future of digital health with federated learning." NPJ Digital Medicine.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Authors

- **Alireza Maleki** - *Initial work* - [@allirezamaleki](https://github.com/allirezamaleki)

## ğŸ™ Acknowledgments

- Medical imaging datasets provided by various healthcare institutions
- Open-source federated learning frameworks that inspired this work
- The medical AI research community for valuable insights and feedback

## ğŸ“ Contact

For questions, suggestions, or collaborations:
- ğŸ“§ Email: [Contact Information]
- ğŸ› Issues: [GitHub Issues](https://github.com/allirezamaleki/Handling-Data-Heterogeneity-in-Medical-Image-Classification/issues)
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/allirezamaleki/Handling-Data-Heterogeneity-in-Medical-Image-Classification/discussions)

---

â­ If you find this project useful, please consider giving it a star on GitHub!
